{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thdGplBzw5Xe"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgsoCDhxL8wX"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqmpHpyDIlPH"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "import datasets\n",
        "timit = datasets.load_dataset(\"crossdelenna/whisper_data_merge2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEBV5p-vIdx-"
      },
      "outputs": [],
      "source": [
        "num_rows = len(timit['train'])\n",
        "num_test_rows = num_rows // 7\n",
        "num_train_rows = num_rows - num_test_rows\n",
        "timit_train = timit[\"train\"].select(range(num_train_rows))\n",
        "timit_test = timit[\"train\"].select(range(num_test_rows))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZg6TeS2MH5J"
      },
      "outputs": [],
      "source": [
        "timit_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtgFoypLM2al"
      },
      "outputs": [],
      "source": [
        "timit_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYAXooWRJJne"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QLXpnFTJdTF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install jiwer librosa evaluate>=0.30 gradio bitsandbytes accelerate\n",
        "from accelerate import PartialState"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVPX-Lkro1Ot"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kODiImxmJ7zD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuaRjtflJ-CM"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "display(IPython.display.Javascript('''\n",
        " function ClickConnect(){\n",
        "   btn = document.querySelector(\"colab-connect-button\")\n",
        "   if (btn != null){\n",
        "     console.log(\"Click colab-connect-button\");\n",
        "     btn.click()\n",
        "     }\n",
        "\n",
        "   btn = document.getElementById('ok')\n",
        "   if (btn != null){\n",
        "     console.log(\"Click reconnect\");\n",
        "     btn.click()\n",
        "     }\n",
        "  }\n",
        "\n",
        "setInterval(ClickConnect,60000)\n",
        "'''))\n",
        "\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFSffHApkAVi"
      },
      "source": [
        "### Load WhisperFeatureExtractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIELrbALjgG-"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"crossdelenna/medium_cross.en\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stPDLMChkKzd"
      },
      "source": [
        "### Load WhisperTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjMiBtavkM9_"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperTokenizer\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"crossdelenna/medium_cross.en\", language=\"English\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnfY-n9PlOsH"
      },
      "source": [
        "### Combine To Create A WhisperProcessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrdVc2y3lPvH"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor\n",
        "processor = WhisperProcessor.from_pretrained(\"crossdelenna/medium_cross.en\", language=\"English\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMbijzjalj_3"
      },
      "source": [
        "Let's print the first example of the Common Voice dataset to see\n",
        "what form the data is in:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmqNXTPnlniJ"
      },
      "outputs": [],
      "source": [
        "print(timit[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKcEWHvKI1by"
      },
      "source": [
        "**Note**: Currently `datasets` make use of [`torchaudio`](https://pytorch.org/audio/stable/index.html) and [`librosa`](https://librosa.org/doc/latest/index.html) for audio loading and resampling. If you wish to implement your own costumized data loading/sampling, feel free to just make use of the `\"path\"` column instead and disregard the `\"audio\"` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tborvC9hx88e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbQf5GuZyQ4_"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO-Zdj-5cxXp"
      },
      "source": [
        "Next, the evaluation metric is defined. As mentioned earlier, the\n",
        "predominant metric in ASR is the word error rate (WER), hence we will use it in this notebook as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Xsux2gmyXso"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "metric = evaluate.load(\"wer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XZ-kjweyTy_"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "    return {\"wer\": wer}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7cqAWIayn6w"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"crossdelenna/medium_cross.en\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4I-YznvIyvZ"
      },
      "source": [
        "# Freeze original layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AckgQu8I1L-"
      },
      "outputs": [],
      "source": [
        "def freeze_whisper_layers(model):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    try:\n",
        "        encoder_layers = model.model.encoder.layers\n",
        "        for layer in encoder_layers[-2:]:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = True\n",
        "    except AttributeError:\n",
        "        print(\"Could not access encoder layers\")\n",
        "    try:\n",
        "        decoder_layers = model.model.decoder.layers\n",
        "        for layer in decoder_layers[-2:]:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = True\n",
        "    except AttributeError:\n",
        "        print(\"Could not access decoder layers\")\n",
        "    try:\n",
        "        model.model.encoder.layer_norm.requires_grad = True\n",
        "    except AttributeError:\n",
        "        print(\"Could not access encoder layer norm\")\n",
        "    try:\n",
        "        model.model.decoder.layer_norm.requires_grad = True\n",
        "    except AttributeError:\n",
        "        print(\"Could not access decoder layer norm\")\n",
        "    for name, module in model.named_children():\n",
        "        if 'proj' in name or 'head' in name or 'classifier' in name:\n",
        "            for param in module.parameters():\n",
        "                param.requires_grad = True\n",
        "    return model\n",
        "\n",
        "model = freeze_whisper_layers(model)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params}\")\n",
        "print(f\"Trainable parameters: {trainable_params}\")\n",
        "print(f\"Percentage of trainable parameters: {trainable_params/total_params*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls9zaYSQnwTV"
      },
      "outputs": [],
      "source": [
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek0TgpuxPgOK"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "maxsteps = 1051\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-medium.en\",\n",
        "    per_device_train_batch_size=22,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=10,\n",
        "    max_steps=maxsteps,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=22,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=350,\n",
        "    eval_steps=350,\n",
        "    logging_steps=350,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        "    hub_strategy=\"every_save\",\n",
        "    hub_model_id=\"crossdelenna/medium_cross.en\",\n",
        "    hub_token = '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',\n",
        ")\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=timit_train,\n",
        "    eval_dataset=timit_test,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")\n",
        "processor.save_pretrained(training_args.output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsW-WZcL1ZtN"
      },
      "source": [
        "Now, all instances can be passed to Trainer and we are ready to start training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIVKJ6LeP69F"
      },
      "outputs": [],
      "source": [
        "# Resume training from a specific checkpoint\n",
        "checkpoint_path = \"crossdelenna/medium_cross.en/model.safetensors\"\n",
        "trainer.train(resume_from_checkpoint=checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O73H-aAwdDZ"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thdGplBzw5Xe"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgsoCDhxL8wX"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqmpHpyDIlPH"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "import datasets\n",
        "timit = datasets.load_dataset(\"crossdelenna/whisper_data_merge2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEBV5p-vIdx-"
      },
      "outputs": [],
      "source": [
        "num_rows = len(timit['train'])\n",
        "num_test_rows = num_rows // 7\n",
        "num_train_rows = num_rows - num_test_rows\n",
        "timit_train = timit[\"train\"].select(range(num_train_rows))\n",
        "timit_test = timit[\"train\"].select(range(num_test_rows))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZg6TeS2MH5J"
      },
      "outputs": [],
      "source": [
        "timit_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtgFoypLM2al"
      },
      "outputs": [],
      "source": [
        "timit_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYAXooWRJJne"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QLXpnFTJdTF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install jiwer librosa evaluate>=0.30 gradio bitsandbytes accelerate\n",
        "from accelerate import PartialState"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVPX-Lkro1Ot"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kODiImxmJ7zD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuaRjtflJ-CM"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "display(IPython.display.Javascript('''\n",
        " function ClickConnect(){\n",
        "   btn = document.querySelector(\"colab-connect-button\")\n",
        "   if (btn != null){\n",
        "     console.log(\"Click colab-connect-button\");\n",
        "     btn.click()\n",
        "     }\n",
        "\n",
        "   btn = document.getElementById('ok')\n",
        "   if (btn != null){\n",
        "     console.log(\"Click reconnect\");\n",
        "     btn.click()\n",
        "     }\n",
        "  }\n",
        "\n",
        "setInterval(ClickConnect,60000)\n",
        "'''))\n",
        "\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFSffHApkAVi"
      },
      "source": [
        "### Load WhisperFeatureExtractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIELrbALjgG-"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"crossdelenna/medium_cross.en\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stPDLMChkKzd"
      },
      "source": [
        "### Load WhisperTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjMiBtavkM9_"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperTokenizer\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"crossdelenna/medium_cross.en\", language=\"English\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnfY-n9PlOsH"
      },
      "source": [
        "### Combine To Create A WhisperProcessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrdVc2y3lPvH"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor\n",
        "processor = WhisperProcessor.from_pretrained(\"crossdelenna/medium_cross.en\", language=\"English\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMbijzjalj_3"
      },
      "source": [
        "Let's print the first example of the Common Voice dataset to see\n",
        "what form the data is in:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmqNXTPnlniJ"
      },
      "outputs": [],
      "source": [
        "print(timit[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKcEWHvKI1by"
      },
      "source": [
        "**Note**: Currently `datasets` make use of [`torchaudio`](https://pytorch.org/audio/stable/index.html) and [`librosa`](https://librosa.org/doc/latest/index.html) for audio loading and resampling. If you wish to implement your own costumized data loading/sampling, feel free to just make use of the `\"path\"` column instead and disregard the `\"audio\"` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tborvC9hx88e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbQf5GuZyQ4_"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO-Zdj-5cxXp"
      },
      "source": [
        "Next, the evaluation metric is defined. As mentioned earlier, the\n",
        "predominant metric in ASR is the word error rate (WER), hence we will use it in this notebook as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Xsux2gmyXso"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "metric = evaluate.load(\"wer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XZ-kjweyTy_"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "    return {\"wer\": wer}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7cqAWIayn6w"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"crossdelenna/medium_cross.en\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4I-YznvIyvZ"
      },
      "source": [
        "# Freeze original layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AckgQu8I1L-"
      },
      "outputs": [],
      "source": [
        "def freeze_whisper_layers(model):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    try:\n",
        "        encoder_layers = model.model.encoder.layers\n",
        "        for layer in encoder_layers[-2:]:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = True\n",
        "    except AttributeError:\n",
        "        print(\"Could not access encoder layers\")\n",
        "    try:\n",
        "        decoder_layers = model.model.decoder.layers\n",
        "        for layer in decoder_layers[-2:]:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = True\n",
        "    except AttributeError:\n",
        "        print(\"Could not access decoder layers\")\n",
        "    try:\n",
        "        model.model.encoder.layer_norm.requires_grad = True\n",
        "    except AttributeError:\n",
        "        print(\"Could not access encoder layer norm\")\n",
        "    try:\n",
        "        model.model.decoder.layer_norm.requires_grad = True\n",
        "    except AttributeError:\n",
        "        print(\"Could not access decoder layer norm\")\n",
        "    for name, module in model.named_children():\n",
        "        if 'proj' in name or 'head' in name or 'classifier' in name:\n",
        "            for param in module.parameters():\n",
        "                param.requires_grad = True\n",
        "    return model\n",
        "\n",
        "model = freeze_whisper_layers(model)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params}\")\n",
        "print(f\"Trainable parameters: {trainable_params}\")\n",
        "print(f\"Percentage of trainable parameters: {trainable_params/total_params*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls9zaYSQnwTV"
      },
      "outputs": [],
      "source": [
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek0TgpuxPgOK"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "maxsteps = 1051\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-medium.en\",\n",
        "    per_device_train_batch_size=22,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=10,\n",
        "    max_steps=maxsteps,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=22,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=350,\n",
        "    eval_steps=350,\n",
        "    logging_steps=350,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        "    hub_strategy=\"every_save\",\n",
        "    hub_model_id=\"crossdelenna/medium_cross.en\",\n",
        " hub_token = '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',\n",
        ")\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=timit_train,\n",
        "    eval_dataset=timit_test,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")\n",
        "processor.save_pretrained(training_args.output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsW-WZcL1ZtN"
      },
      "source": [
        "Now, all instances can be passed to Trainer and we are ready to start training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIVKJ6LeP69F"
      },
      "outputs": [],
      "source": [
        "# Resume training from a specific checkpoint\n",
        "checkpoint_path = \"crossdelenna/medium_cross.en/model.safetensors\"\n",
        "trainer.train(resume_from_checkpoint=checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O73H-aAwdDZ"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
