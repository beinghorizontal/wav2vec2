{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wav2Vec2_Finetune_EN_IN_DirectData.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beinghorizontal/wav2vec2/blob/main/Wav2Vec2_Finetune_EN_IN_DirectData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "import datasets\n",
        "timit = datasets.load_dataset(\"crossdelenna/en_in2\", use_auth_token='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')\n"
      ],
      "metadata": {
        "id": "kUlZBNB4czNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBSYoWbi-45k"
      },
      "source": [
        "# **Fine-tuning Wav2Vec2 for English ASR with ðŸ¤— Transformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT_QrfWtsxIz"
      },
      "source": [
        "In this notebook, we will give an in-detail explanation of how Wav2Vec2's pretrained checkpoints can be fine-tuned on any English ASR dataset. Note that in this notebook, we will fine-tune Wav2Vec2 without making use of a language model. It is much simpler to use Wav2Vec2 without a language model as an end-to-end ASR system and it has been shown that a standalone Wav2Vec2 acoustic model achieves impressive results. For demonstration purposes, we fine-tune the \"base\"-sized [pretrained checkpoint](https://huggingface.co/facebook/wav2vec2-base) on my personal labeled dataset that contains small training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx9OdDYrCtQ1"
      },
      "source": [
        "Wav2Vec2 is fine-tuned using Connectionist Temporal Classification (CTC), which is an algorithm that is used to train neural networks for sequence-to-sequence problems and mainly in Automatic Speech Recognition and handwriting recognition. \n",
        "\n",
        "I highly recommend reading the blog post [Sequence Modeling with CTC (2017)](https://distill.pub/2017/ctc/) very well-written blog post by Awni Hannun."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW3J3rBizeds"
      },
      "source": [
        "First, let's try to get a good GPU in our colab! With Google Colab's free version it's sadly becoming much harder to get access to a good GPU. With Google Colab Pro, one has a much easier time getting access to a V100 or P100 GPU however."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLAufgh_xxj7"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import from Drive\n",
        "\n",
        "Mode = \"checkpoints\" #@param [\"workspace\", \"data_src\", \"data_dst\", \"data_src aligned\", \"data_dst aligned\", \"models\"]\n",
        "Archive_name = \"checkpoints.zip\" #@param {type:\"string\"}\n",
        "\n",
        "#Mount Google Drive as folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!unzip \"/content/drive/MyDrive/checkpoints.zip\" -d \"/content/\"\n",
        "# def zip_and_copy(path, mode):\n",
        "#   unzip_cmd=\" -q \"+Archive_name\n",
        "  \n",
        "#   %cd $path\n",
        "#   copy_cmd = \"/content/drive/My\\ Drive/\"+Archive_name+\" \"+path\n",
        "#   !cp $copy_cmd\n",
        "#   !unzip $unzip_cmd    \n",
        "#   !rm $Archive_name\n",
        "\n",
        "# zip_and_copy(\"/content\", \"mycheckpoints\")\n",
        "  \n",
        "print(\"Done!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "3iNBKQQAPCz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e335hPmdtASZ"
      },
      "source": [
        "Before we start, let's install both `datasets` and `transformers` from master. Also, we need the `librosa` package to load audio files and the `jiwer` to evaluate our fine-tuned model using the [word error rate (WER)](https://huggingface.co/metrics/wer) metric ${}^1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8eh87Hoee5d"
      },
      "source": [
        "%%capture\n",
        "!pip install datasets==1.18.3\n",
        "!pip install transformers==4.17.0\n",
        "!pip install jiwer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "qEGNtPmct723"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path to upload checkpoints to drive\n",
        "from shutil import copyfile\n",
        "dst = '/usr/local/lib/python3.7/dist-packages/transformers/trainer.py'\n",
        "src = '/content/drive/MyDrive/trainer_mod.py'\n",
        "copyfile(src, dst)"
      ],
      "metadata": {
        "id": "Jn71uprieQTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_6kYmDMH9lR"
      },
      "source": [
        "Next we strongly suggest to upload your training checkpoints directly to the [ðŸ¤— Hub](https://huggingface.co/) while training. The [ðŸ¤— Hub](https://huggingface.co/) has integrated version control so you can be sure that no model checkpoint is getting lost during training. \n",
        "\n",
        "To do so you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCyw5D23IQ1F"
      },
      "source": [
        "\n",
        "Then you need to install Git-LFS to upload your model checkpoints:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9BnQDhOITBC"
      },
      "source": [
        "%%capture\n",
        "!apt install git-lfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prevent random disconnects\n",
        "\n",
        "This cell runs JS code to automatic reconnect to runtime."
      ],
      "metadata": {
        "id": "uRIsgKjEAwp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "display(IPython.display.Javascript('''\n",
        " function ClickConnect(){\n",
        "   btn = document.querySelector(\"colab-connect-button\")\n",
        "   if (btn != null){\n",
        "     console.log(\"Click colab-connect-button\"); \n",
        "     btn.click() \n",
        "     }\n",
        "   \n",
        "   btn = document.getElementById('ok')\n",
        "   if (btn != null){\n",
        "     console.log(\"Click reconnect\"); \n",
        "     btn.click() \n",
        "     }\n",
        "  }\n",
        "  \n",
        "setInterval(ClickConnect,60000)\n",
        "'''))\n",
        "\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "OZW5U6oPAxn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get data from HF repo (private)"
      ],
      "metadata": {
        "id": "U54NEwXLnbDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import IPython.display as ipd\n",
        "# import numpy as np\n",
        "# import random\n",
        "\n",
        "# rand_int = random.randint(0, len(timit[\"train\"]))\n",
        "\n",
        "# print(timit[\"train\"][rand_int][\"text\"])\n",
        "# ipd.Audio(data=np.asarray(timit[\"train\"][rand_int][\"audio\"][\"array\"]), autoplay=True, rate=16000)\n",
        "# timit['train']['input_values'][2]\n",
        "timit\n"
      ],
      "metadata": {
        "id": "t-28XBEwgJsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mW-C1Nt-j7k"
      },
      "source": [
        "## Prepare Data, Tokenizer, Feature Extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeBosnY9BH3e"
      },
      "source": [
        "ASR models transcribe speech to text, which means that we both need a feature extractor that processes the speech signal to the model's input format, *e.g.* a feature vector, and a tokenizer that processes the model's output format to text. \n",
        "\n",
        "In ðŸ¤— Transformers, the Wav2Vec2 model is thus accompanied by both a tokenizer, called [Wav2Vec2CTCTokenizer](https://huggingface.co/transformers/master/model_doc/wav2vec2.html#wav2vec2ctctokenizer), and a feature extractor, called [Wav2Vec2FeatureExtractor](https://huggingface.co/transformers/master/model_doc/wav2vec2.html#wav2vec2featureextractor).\n",
        "\n",
        "Let's start by creating the tokenizer responsible for decoding the model's predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEXEWEJGQPqD"
      },
      "source": [
        "### Create Wav2Vec2CTCTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWmMikuNEKl_"
      },
      "source": [
        "The [pretrained Wav2Vec2 checkpoint]( ) maps the speech signal to a sequence of context representations as illustrated in the figure above. A fine-tuned Wav2Vec2 checkpoint needs to map this sequence of context representations to its corresponding transcription so that a linear layer has to be added on top of the transformer block (shown in yellow). This linear layer is used to classifies each context representation to a token class analogous how, *e.g.*, after pretraining a linear layer is added on top of BERT's embeddings for further classification - *cf.* with *\"BERT\"* section of this [blog post](https://huggingface.co/blog/warm-starting-encoder-decoder).\n",
        "\n",
        "The output size of this layer corresponds to the number of tokens in the vocabulary, which does **not** depend on Wav2Vec2's pretraining task, but only on the labeled dataset used for fine-tuning. So in the first step, we will take a look at Timit and define a vocabulary based on the dataset's transcriptions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bee4g9rpLxll"
      },
      "source": [
        "Let's start by loading the dataset and taking a look at its structure."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2FeatureExtractor, Wav2Vec2Processor, Wav2Vec2CTCTokenizer\n",
        "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
        "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"crossdelenna/wav2vec2-base-en-in\")\n",
        "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "xWVh3_F6rgK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_dict = tokenizer.get_vocab()\n",
        "#sort_vocab = sorted((value, key) for (key,value) in vocab_dict.items())\n",
        "len(vocab_dict)\n",
        "\n"
      ],
      "metadata": {
        "id": "KHbX9HlNgTyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get data from /content that drive extracted to create HF compatible data (here gave the name timit)"
      ],
      "metadata": {
        "id": "arV2qxhELDYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length_in_sec = 13.0\n",
        "timit[\"train\"] = timit[\"train\"].filter(lambda x: x < max_input_length_in_sec * processor.feature_extractor.sampling_rate, input_columns=[\"input_length\"])"
      ],
      "metadata": {
        "id": "iGV5r1TWL6HF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYcIiR2FQ96i"
      },
      "source": [
        "### Create Wav2Vec2 Feature Extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6mDEyW719rx"
      },
      "source": [
        "Speech is a continuous signal and to be treated by computers, it first has to be discretized, which is usually called **sampling**. The sampling rate hereby plays an important role in that it defines how many data points of the speech signal are measured per second. Therefore, sampling with a higher sampling rate results in a better approximation of the *real* speech signal but also necessitates more values per second.\n",
        "\n",
        "A pretrained checkpoint expects its input data to have been sampled more or less from the same distribution as the data it was trained on. The same speech signals sampled at two different rates have a very different distribution, *e.g.*, doubling the sampling rate results in data points being twice as long. Thus, \n",
        "before fine-tuning a pretrained checkpoint of an ASR model, it is crucial to verify that the sampling rate of the data that was used to pretrain the model matches the sampling rate of the dataset used to fine-tune the model.\n",
        "\n",
        "Wav2Vec2 was pretrained on the audio data of [LibriSpeech](https://huggingface.co/datasets/librispeech_asr) and LibriVox which both were sampling with 16kHz.This why I also sampled with 16kHz. If the fine-tuning dataset would have been sampled with a rate lower or higher than 16kHz, we first would have had to up or downsample the speech signal to match the sampling rate of the data used for pretraining. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuUbPW7oV-B5"
      },
      "source": [
        "A Wav2Vec2 feature extractor object requires the following parameters to be instantiated:\n",
        "\n",
        "- `feature_size`: Speech models take a sequence of feature vectors as an input. While the length of this sequence obviously varies, the feature size should not. In the case of Wav2Vec2, the feature size is 1 because the model was trained on the raw speech signal ${}^2$.\n",
        "- `sampling_rate`: The sampling rate at which the model is trained on.\n",
        "- `padding_value`: For batched inference, shorter inputs need to be padded with a specific value\n",
        "- `do_normalize`: Whether the input should be *zero-mean-unit-variance* normalized or not. Usually, speech models perform better when normalizing the input\n",
        "- `return_attention_mask`: Whether the model should make use of an `attention_mask` for batched inference. In general, models should **always** make use of the `attention_mask` to mask padded tokens. However, due to a very specific design choice of `Wav2Vec2`'s \"base\" checkpoint, better results are achieved when using no `attention_mask`. This is **not** recommended for other speech models. For more information, one can take a look at [this](https://github.com/pytorch/fairseq/issues/3227) issue. **Important** If you want to use this notebook to fine-tune [large-lv60](https://huggingface.co/facebook/wav2vec2-large-lv60), this parameter should be set to `True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mbffBdxIl0M"
      },
      "source": [
        "repo_name = \"wav2vec2-base-en-in\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvL12DrNV4cx"
      },
      "source": [
        "Check the files it created under my repository [Here](https://huggingface.co/beinghorizontal/wav2vec2-base-en-in/tree/main)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4J0bU1WsvAg"
      },
      "source": [
        "Long input sequences require a lot of memory. Since `Wav2Vec2` is based on `self-attention` the memory requirement scales quadratically with the input length for long input sequences (*cf.* with [this](https://www.reddit.com/r/MachineLearning/comments/genjvb/d_why_is_the_maximum_input_sequence_length_of/) reddit post). For this demo, let's filter all sequences that are longer than 4 seconds out of the training dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25Genil2v_Br"
      },
      "source": [
        "Awesome, now we are ready to start training!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYlQkKVoRUos"
      },
      "source": [
        "## Training & Evaluation\n",
        "\n",
        "The data is processed so that we are ready to start setting up the training pipeline. We will make use of ðŸ¤—'s [Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer) for which we essentially need to do the following:\n",
        "\n",
        "- Define a data collator. In contrast to most NLP models, Wav2Vec2 has a much larger input length than output length. *E.g.*, a sample of input length 50000 has an output length of no more than 100. Given the large input sizes, it is much more efficient to pad the training batches dynamically meaning that all training samples should only be padded to the longest sample in their batch and not the overall longest sample. Therefore, fine-tuning Wav2Vec2 requires a special padding data collator, which we will define below\n",
        "\n",
        "- Evaluation metric. During training, the model should be evaluated on the word error rate. We should define a `compute_metrics` function accordingly\n",
        "\n",
        "- Load a pretrained checkpoint. We need to load a pretrained checkpoint and configure it correctly for training.\n",
        "\n",
        "- Define the training configuration.\n",
        "\n",
        "After having fine-tuned the model, we will correctly evaluate it on the test data and verify that it has indeed learned to correctly transcribe speech."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slk403unUS91"
      },
      "source": [
        "### Set-up Trainer\n",
        "\n",
        "Let's start by defining the data collator. The code for the data collator was copied from [this example](https://github.com/huggingface/transformers/blob/9a06b6b11bdfc42eea08fa91d0c737d1863c99e3/examples/research_projects/wav2vec2/run_asr.py#L81).\n",
        "\n",
        "Without going into too many details, in contrast to the common data collators, this data collator treats the `input_values` and `labels` differently and thus applies to separate padding functions on them (again making use of Wav2Vec2's context manager). This is necessary because in speech input and output are of different modalities meaning that they should not be treated by the same padding function.\n",
        "Analogous to the common data collators, the padding tokens in the labels with `-100` so that those tokens are **not** taken into account when computing the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tborvC9hx88e"
      },
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received.\n",
        "    Args:\n",
        "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
        "            The processor used for proccessing the data.\n",
        "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
        "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
        "            among:\n",
        "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
        "              sequence if provided).\n",
        "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
        "              maximum acceptable input length for the model if that argument is not provided.\n",
        "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
        "              different lengths).\n",
        "    \"\"\"\n",
        "\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lenghts and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                label_features,\n",
        "                padding=self.padding,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ## Data collator for training "
      ],
      "metadata": {
        "id": "u97R_0OH3tPc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbQf5GuZyQ4_"
      },
      "source": [
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO-Zdj-5cxXp"
      },
      "source": [
        "Next, the evaluation metric is defined. As mentioned earlier, the \n",
        "predominant metric in ASR is the word error rate (WER), hence we will use it in this notebook as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xsux2gmyXso"
      },
      "source": [
        "from datasets import load_metric\n",
        "wer_metric = load_metric(\"wer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1qZU5p-deqB"
      },
      "source": [
        "The model will return a sequence of logit vectors:\n",
        "$\\mathbf{y}_1, \\ldots, \\mathbf{y}_m$ with $\\mathbf{y}_1 = f_{\\theta}(x_1, \\ldots, x_n)[0]$ and $n >> m$.\n",
        "\n",
        "A logit vector $\\mathbf{y}_1$ contains the log-odds for each word in the vocabulary we defined earlier, thus $\\text{len}(\\mathbf{y}_i) =$ `config.vocab_size`. We are interested in the most likely prediction of the model and thus take the `argmax(...)` of the logits. Also, we transform the encoded labels back to the original string by replacing `-100` with the `pad_token_id` and decoding the ids while making sure that consecutive tokens are **not** grouped to the same token in CTC style ${}^1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XZ-kjweyTy_"
      },
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmgrx4bRwLIH"
      },
      "source": [
        "Now, we can load the pretrained `Wav2Vec2` checkpoint. The tokenizer's `pad_token_id` must be to define the model's `pad_token_id` or in the case of `Wav2Vec2ForCTC` also CTC's *blank token* ${}^2$. To save GPU memory, we enable PyTorch's [gradient checkpointing](https://pytorch.org/docs/stable/checkpoint.html) and also set the loss reduction to \"*mean*\"."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import model from storage"
      ],
      "metadata": {
        "id": "3nKtiaWHOvGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# dics = os.listdir('/content/drive/MyDrive/wav2vec2-base-en-in/wav2vec2-base-en-in')\n",
        "dics = os.listdir('/content/wav2vec2-base-en-in')\n",
        "match = [t for t in dics if 'checkpoint' in t]\n",
        "model_url = '/content/wav2vec2-base-en-in/'+match[0]\n",
        "print(model_url)"
      ],
      "metadata": {
        "id": "6Q7ut_28i-vT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7cqAWIayn6w"
      },
      "source": [
        "from transformers import Wav2Vec2ForCTC\n",
        "#output_dir=\"crossdelenna/wav2vec2-base-en-in\"  # Default\n",
        "output_dir = model_url # \"/content/wav2vec2-base-en-in/checkpoint-40000\"  # drive /content/content/wav2vec2-base-en-in\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    output_dir,\n",
        "    attention_dropout=0.05,\n",
        "    hidden_dropout=0.05,\n",
        "    feat_proj_dropout=0.05,\n",
        "    mask_time_prob=0.05,\n",
        "    layerdrop=0.0,\n",
        "    ctc_loss_reduction=\"mean\", \n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        "    vocab_size=len(processor.tokenizer)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DwR3XLSzGDD"
      },
      "source": [
        "The first component of Wav2Vec2 consists of a stack of CNN layers that are used to extract acoustically meaningful - but contextually independent - features from the raw speech signal. This part of the model has already been sufficiently trained during pretrainind and as stated in the [paper](https://arxiv.org/abs/2006.11477) does not need to be fine-tuned anymore. \n",
        "Thus, we can set the `requires_grad` to `False` for all parameters of the *feature extraction* part."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.freeze_feature_encoder()"
      ],
      "metadata": {
        "id": "Et_NUAZjWppc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD4aGhQM0K-D"
      },
      "source": [
        "In a final step, we define all parameters related to training. \n",
        "To give more explanation on some of the parameters:\n",
        "- `group_by_length` makes training more efficient by grouping training samples of similar input length into one batch. This can significantly speed up training time by heavily reducing the overall number of useless padding tokens that are passed through the model\n",
        "- `learning_rate` and `weight_decay` were heuristically tuned until fine-tuning has become stable. Note that those parameters strongly depend on the Timit dataset and might be suboptimal for other speech datasets.\n",
        "\n",
        "For more explanations on other parameters, one can take a look at the [docs](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer#trainingarguments).\n",
        "\n",
        "During training, a checkpoint will be uploaded asynchronously to the hub every 400 training steps. It allows you to also play around with the demo widget even while your model is still training.\n",
        "\n",
        "**Note**: If one does not want to upload the model checkpoints to the hub, simply set `push_to_hub=False`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbeKSV7uzGPP"
      },
      "source": [
        "from transformers import TrainingArguments\n",
        "#output_dir=repo_name\n",
        "output_dir = \"/content/wav2vec2-base-en-in\"  # drive\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=output_dir,\n",
        "  push_to_hub=False,\n",
        "  group_by_length=True,\n",
        "  per_device_train_batch_size=8,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=300,\n",
        "  fp16=True,\n",
        "  gradient_checkpointing=True,\n",
        "  save_steps=400,\n",
        "  eval_steps=400,\n",
        "  logging_steps=400,\n",
        "  learning_rate=1e-4,  # default -4\n",
        "  #weight_decay=0.005,  # default 0.005\n",
        "  warmup_steps=1000,  # default 1000, it increases learning rate from low to high after each steps mentioned\n",
        "  save_total_limit=1,\n",
        "  hub_token = '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'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import inspect\n",
        "from transformers import trainer\n",
        "\n",
        "package_path = os.path.dirname(inspect.getfile(trainer))\n",
        "print(package_path)"
      ],
      "metadata": {
        "id": "DGlZeCVHTnhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsW-WZcL1ZtN"
      },
      "source": [
        "Now, all instances can be passed to Trainer and we are ready to start training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY7vBmFCPFgC"
      },
      "source": [
        "import sys\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=timit[\"train\"],\n",
        "    eval_dataset=timit[\"test\"],\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoXBx1JAA0DX"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "${}^1$ To allow models to become independent of the speaker rate, in CTC, consecutive tokens that are identical are simply grouped as a single token. However, the encoded labels should not be grouped when decoding since they don't correspond to the predicted tokens of the model, which is why the `group_tokens=False` parameter has to be passed. If we wouldn't pass this parameter a word like `\"hello\"` would incorrectly be encoded, and decoded as `\"helo\"`.\n",
        "\n",
        "${}^2$ The blank token allows the model to predict a word, such as `\"hello\"` by forcing it to insert the blank token between the two l's. A CTC-conform prediction of `\"hello\"` of our model would be `[PAD] [PAD] \"h\" \"e\" \"e\" \"l\" \"l\" [PAD] \"l\" \"o\" \"o\" [PAD]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpvZHM1xReIW"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-3oKSzZ1hGq"
      },
      "source": [
        "Training will take between 90 and 270 minutes depending on the GPU allocated to this notebook. While the trained model yields satisfying results on *Timit*'s test data, it is by no means an optimally fine-tuned model. The purpose of this notebook is to demonstrate how Wav2Vec2's [base](https://huggingface.co/facebook/wav2vec2-base), [large](https://huggingface.co/facebook/wav2vec2-large), and [large-lv60](https://huggingface.co/facebook/wav2vec2-large-lv60) checkpoints can be fine-tuned on any English dataset.\n",
        "\n",
        "In case you want to use this google colab to fine-tune your model, you should make sure that your training doesn't stop due to inactivity. A simple hack to prevent this is to paste the following code into the console of this tab (*right mouse click -> inspect -> Console tab and insert code*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYYAvgkW4P0m"
      },
      "source": [
        "```javascript\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\"); \n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmY6yqd1zvzU"
      },
      "source": [
        "Depending on what GPU was allocated to your google colab it might be possible that you are seeing an `\"out-of-memory\"` error here. In this case, it's probably best to reduce `per_device_train_batch_size` to 16 or even less and eventually make use of [`gradient_accumulation`](https://huggingface.co/transformers/master/main_classes/trainer.html#trainingarguments)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UEjJqGsQw24"
      },
      "source": [
        "trainer.train(resume_from_checkpoint=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCyp-v3n4Zlt"
      },
      "source": [
        "The final WER should be around 0.3 which is reasonable given that state-of-the-art phoneme error rates (PER) are just below 0.1 (see [leaderboard](https://paperswithcode.com/sota/speech-recognition-on-timit)) and that WER is usually worse than PER.\n",
        "\n",
        "You can now upload the result of the training to the Hub, just execute this instruction:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Export to Drive { form-width: \"30%\" }\n",
        "# Mode = \"checkpoints\" #@param [\"checkpoints\", \"data_src\", \"data_dst\", \"data_src aligned\", \"data_dst aligned\", \"merged\", \"merged_mask\", \"models\", \"result video\", \"result_mask video\"]\n",
        "# Archive_name = \"checkpoints.zip\" #@param {type:\"string\"}\n",
        "\n",
        "# #Mount Google Drive as folder\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# def zip_and_copy(path, mode):\n",
        "#   zip_cmd=\"-r -q \"+Archive_name+\" \"\n",
        "  \n",
        "#   %cd $path\n",
        "#   zip_cmd+=mode\n",
        "#   !zip $zip_cmd\n",
        "#   copy_cmd = \" \"+Archive_name+\"  /content/drive/My\\ Drive/\"\n",
        "#   !cp $copy_cmd\n",
        "#   !rm $Archive_name\n",
        "\n",
        "# zip_and_copy(\"/content\", \"wav2vec2-base-en-in\")\n",
        "  \n",
        "# print(\"Done!\")\n"
      ],
      "metadata": {
        "id": "4x-vg-zZIbGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python3 /content/drive/MyDrive/zipper.py"
      ],
      "metadata": {
        "id": "t4fuwjdvH3aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this if you want to push existing model to huggingface and to avoid git error of cloning in existing folder\n",
        "#if fails try cd in to wav2vec2-base-en-in folder first \n",
        "\n",
        "from transformers import TrainingArguments\n",
        "#output_dir=repo_name\n",
        "output_dir = \"/content/wav2vec2-base-en-in/wav2vec2-base-en-in2\"  # drive\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=output_dir,\n",
        "  push_to_hub=False,\n",
        "  group_by_length=True,\n",
        "  per_device_train_batch_size=8,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=300,\n",
        "  fp16=True,\n",
        "  gradient_checkpointing=True,\n",
        "  save_steps=400,\n",
        "  eval_steps=400,\n",
        "  logging_steps=400,\n",
        "  learning_rate=1e-6,  # default -4\n",
        "  weight_decay=0.008,  # default 0.005\n",
        "  warmup_steps=1000,  # default 1000, it increases learning rate from low to high after each steps mentioned\n",
        "  save_total_limit=1,\n",
        "  hub_token = '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'\n",
        ")\n",
        "\n",
        "import os\n",
        "import inspect\n",
        "from transformers import trainer\n",
        "\n",
        "package_path = os.path.dirname(inspect.getfile(trainer))\n",
        "print(package_path)\n",
        "\n",
        "import sys\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=timit[\"train\"],\n",
        "    eval_dataset=timit[\"test\"],\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")\n"
      ],
      "metadata": {
        "id": "jnGcUwiCIDUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYGQYBhHNsvj"
      },
      "source": [
        "trainer.push_to_hub()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djzwS5WeNu16"
      },
      "source": [
        "You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier \"your-username/the-name-you-picked\" so for instance:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adm0LngNNxq7"
      },
      "source": [
        "```python\n",
        "from transformers import AutoModelForCTC, Wav2Vec2Processor\n",
        "\n",
        "model = AutoModelForCTC.from_pretrained(\"patrickvonplaten/wav2vec2-base-timit-demo-google-colab\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"patrickvonplaten/wav2vec2-base-timit-demo-google-colab\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD4XP5IHncsY"
      },
      "source": [
        "### Evaluate\n",
        "\n",
        "In the final part, we run our model on some of the validation data to get a feeling for how well it works.\n",
        "\n",
        "Let's load the `processor` and `model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rg0USDCnbo6"
      },
      "source": [
        "#processor = Wav2Vec2Processor.from_pretrained(\"crossdelenna/wav2vec2-base-en-in\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSpFW3pDn23P"
      },
      "source": [
        "#model = Wav2Vec2ForCTC.from_pretrained(\"beinghorizontal/wav2vec2-base-en-in\").cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUuZpB0v5jn_"
      },
      "source": [
        "Now, we will make use of the `map(...)` function to predict the transcription of every test sample and to save the prediction in the dataset itself. We will call the resulting dictionary `\"results\"`. \n",
        "\n",
        "**Note**: we evaluate the test data set with `batch_size=1` on purpose due to this [issue](https://github.com/pytorch/fairseq/issues/3227). Since padded inputs don't yield the exact same output as non-padded inputs, a better WER can be achieved by not padding the input at all."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40naJl53n7jT"
      },
      "source": [
        "\"\"\"\n",
        "def map_to_result(batch):\n",
        "  with torch.no_grad():\n",
        "    input_values = torch.tensor(batch[\"input_values\"], device=\"cuda\").unsqueeze(0)\n",
        "    logits = model(input_values).logits\n",
        "\n",
        "  pred_ids = torch.argmax(logits, dim=-1)\n",
        "  batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
        "  batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
        "  \n",
        "  return batch\n",
        "  \"\"\"\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPqH7gZqrGPi"
      },
      "source": [
        "#results = timit[\"test\"].map(map_to_result, remove_columns=timit[\"test\"].column_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mqpdB8R6rty"
      },
      "source": [
        "Let's compute the overall WER now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmqAb4Isx8OK"
      },
      "source": [
        "#print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"text\"])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Va94d8Y7Q98"
      },
      "source": [
        "21.8% WER - not bad! Our demo model would have probably made it on the official [leaderboard](https://paperswithcode.com/sota/speech-recognition-on-timit).\n",
        "\n",
        "Let's take a look at some predictions to see what errors are made by the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from datasets import ClassLabel\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    display(HTML(df.to_html()))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cJphEIy0POY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odNDiFVRy53w"
      },
      "source": [
        "#show_random_elements(results)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HFCujhd9n4N"
      },
      "source": [
        "It becomes clear that the predicted transcriptions are acoustically very similar to the target transcriptions, but often contain spelling or grammatical errors. This shouldn't be very surprising though given that we purely rely on Wav2Vec2 without making use of a language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3ydKvUl9FTK"
      },
      "source": [
        "Finally, to better understand how CTC works, it is worth taking a deeper look at the exact output of the model. Let's run the first test sample through the model, take the predicted ids and convert them to their corresponding tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqaM45t87uM4"
      },
      "source": [
        "\"\"\"\n",
        "model.to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  logits = model(torch.tensor(timit[\"test\"][:1][\"input_values\"], device=\"cuda\")).logits\n",
        "\n",
        "pred_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "# convert ids to tokens\n",
        "\" \".join(processor.tokenizer.convert_ids_to_tokens(pred_ids[0].tolist()))\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdO8E28g-n5C"
      },
      "source": [
        "The output should make it a bit clearer how CTC works in practice. The model is to some extent invariant to speaking rate since it has learned to either just repeat the same token in case the speech chunk to be classified still corresponds to the same token. This makes CTC a very powerful algorithm for speech recognition since the speech file's transcription is often very much independent of its length.\n",
        "\n",
        "I again advise the reader to take a look at [this](https://distill.pub/2017/ctc) very nice blog post to better understand CTC."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Export to Drive { form-width: \"30%\" }\n",
        "#Mode = \"my_checkpoints\" #@param [\"workspace\", \"data_src\", \"data_dst\", \"data_src aligned\", \"data_dst aligned\", \"merged\", \"merged_mask\", \"models\", \"result video\", \"result_mask video\"]\n",
        "#Archive_name = \"my_checkpoints.zip\" #@param {type:\"string\"}\n",
        "\n",
        "#Mount Google Drive as folder\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive', force_remount=True)\n",
        "#!cp -av /content/drive/MyDrive/wav2vec2-base-en-in/wav2vec2-base-en-in /content/wav2vec2-base-en-in\n",
        "#!cp -av /content/content/wav2vec2-base-en-in /content/wav2vec2-base-en-in\n",
        "\n",
        "# /content/content/wav2vec2-base-en-in\n",
        "# /content/wav2vec2-base-en-in"
      ],
      "metadata": {
        "id": "BG_0wrSrwipV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DQKzEb6hQmZ3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}