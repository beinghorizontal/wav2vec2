{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "afe29f03f21a468c9de93db821df7119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_edad89c98e7049d1a10bf0ac21554a2e",
              "IPY_MODEL_d4852d7defb346048f0cd37868e0f80e",
              "IPY_MODEL_aff3eef26db848b3ab7cf7e2cf114f80"
            ],
            "layout": "IPY_MODEL_aef5a276f519430bbeed5bba93db573f"
          }
        },
        "edad89c98e7049d1a10bf0ac21554a2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6df33a955217448794e957594804a244",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_92574db1765742c0a8ca2d3b938089a9",
            "value": "events.out.tfevents.1739240955.4cee2e63b939.6105.0:â€‡100%"
          }
        },
        "d4852d7defb346048f0cd37868e0f80e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0b2ae3bac694c57b41efb30fcaf771b",
            "max": 7846,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18b6427a7b484e65aca93ec603484632",
            "value": 7846
          }
        },
        "aff3eef26db848b3ab7cf7e2cf114f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fde9756055c64286b6bbede5bcf427bb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_32eb2c8bb97f44cf99baa29b47b92b0d",
            "value": "â€‡7.85k/7.85kâ€‡[00:00&lt;00:00,â€‡25.0kB/s]"
          }
        },
        "aef5a276f519430bbeed5bba93db573f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6df33a955217448794e957594804a244": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92574db1765742c0a8ca2d3b938089a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0b2ae3bac694c57b41efb30fcaf771b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18b6427a7b484e65aca93ec603484632": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fde9756055c64286b6bbede5bcf427bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32eb2c8bb97f44cf99baa29b47b92b0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beinghorizontal/wav2vec2/blob/main/finetune_crossdelenna_medium_cross_en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install required packages\n"
      ],
      "metadata": {
        "id": "ILN_e_Hoq08R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.system('pip install datasets transformers jiwer evaluate huggingface_hub tokenizers')\n"
      ],
      "metadata": {
        "id": "0v4652Rsq3Qk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "090b677c-5bc9-49ba-8fd6-62a49b3835af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import random\n",
        "import librosa\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "from transformers import (\n",
        "    WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor,\n",
        "    WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        ")\n",
        "from google.colab import drive, output\n"
      ],
      "metadata": {
        "id": "tYL6iMsTqkzG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enable custom widget manager\n"
      ],
      "metadata": {
        "id": "9aCiKJjKqnzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output.enable_custom_widget_manager()\n"
      ],
      "metadata": {
        "id": "wdysm0h2qrj_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check GPU availability\n"
      ],
      "metadata": {
        "id": "Dl499udLqtq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = os.popen('nvidia-smi').read()\n",
        "if 'failed' in gpu_info:\n",
        "    print('Not connected to a GPU')\n",
        "else:\n",
        "    print(gpu_info)\n"
      ],
      "metadata": {
        "id": "cqkDMLuOqx8Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a2650ba-84e6-47c4-f59f-796cd209d2c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Feb 11 02:28:08 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset"
      ],
      "metadata": {
        "id": "Jcdd6sAGq7H8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "timit = datasets.load_dataset(\"crossdelenna/whisper_data_merge2\")\n"
      ],
      "metadata": {
        "id": "_Dw_CMA9q9u4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84d15047-e1fb-4533-b459-a59d3d2b5655"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split dataset\n"
      ],
      "metadata": {
        "id": "GT2Ou0_srBxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_rows = len(timit['train'])\n",
        "num_test_rows = num_rows // 7\n",
        "num_train_rows = num_rows - num_test_rows\n",
        "timit_train = timit[\"train\"].select(range(num_train_rows))\n",
        "timit_test = timit[\"train\"].select(range(num_test_rows))\n"
      ],
      "metadata": {
        "id": "t081hsDyrFmN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Whisper components from Hugging Face Hub\n"
      ],
      "metadata": {
        "id": "9HIZkRUfrKYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"crossdelenna/whisper_med_alex.en\")\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"crossdelenna/whisper_med_alex.en\", language=\"English\", task=\"transcribe\")\n",
        "processor = WhisperProcessor.from_pretrained(\"crossdelenna/whisper_med_alex.en\", language=\"English\", task=\"transcribe\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"crossdelenna/whisper_med_alex.en\")\n"
      ],
      "metadata": {
        "id": "xCPKqQqGrOnK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data collator\n"
      ],
      "metadata": {
        "id": "yUsyaFgQrRWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n"
      ],
      "metadata": {
        "id": "EH11dxpnrVEa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation metric\n"
      ],
      "metadata": {
        "id": "3uHZyQSnrX-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"wer\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "    return {\"wer\": wer}\n"
      ],
      "metadata": {
        "id": "ZYAcKqV2rbBS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Freeze layers\n"
      ],
      "metadata": {
        "id": "iDhp5NLWrdTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_whisper_layers(model):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    try:\n",
        "        encoder_layers = model.model.encoder.layers\n",
        "        for layer in encoder_layers[-2:]:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = True\n",
        "    except AttributeError:\n",
        "        print(\"Could not access encoder layers\")\n",
        "\n",
        "    try:\n",
        "        decoder_layers = model.model.decoder.layers\n",
        "        for layer in decoder_layers[-2:]:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = True\n",
        "    except AttributeError:\n",
        "        print(\"Could not access decoder layers\")\n",
        "\n",
        "    try:\n",
        "        model.model.encoder.layer_norm.requires_grad = True\n",
        "    except AttributeError:\n",
        "        print(\"Could not access encoder layer norm\")\n",
        "\n",
        "    try:\n",
        "        model.model.decoder.layer_norm.requires_grad = True\n",
        "    except AttributeError:\n",
        "        print(\"Could not access decoder layer norm\")\n",
        "\n",
        "    for name, module in model.named_children():\n",
        "        if 'proj' in name or 'head' in name or 'classifier' in name:\n",
        "            for param in module.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "    return model\n",
        "\n",
        "model = freeze_whisper_layers(model)"
      ],
      "metadata": {
        "id": "CncGzhDDrhEl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verify trainable parameters\n"
      ],
      "metadata": {
        "id": "UbwtIjUor480"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total parameters: {total_params}\")\n",
        "print(f\"Trainable parameters: {trainable_params}\")\n",
        "print(f\"Percentage of trainable parameters: {trainable_params/total_params*100:.2f}%\")"
      ],
      "metadata": {
        "id": "sgZXKBHFr3ZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dab9785-2aac-4673-80cf-6046d4e509f2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 763856896\n",
            "Trainable parameters: 111888384\n",
            "Percentage of trainable parameters: 14.65%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training arguments\n"
      ],
      "metadata": {
        "id": "GBYnd1f5r-go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-medium.en\",\n",
        "    per_device_train_batch_size=24,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=10,\n",
        "    max_steps=901,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=16,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=300,\n",
        "    eval_steps=300,\n",
        "    logging_steps=300,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        "    hub_strategy=\"checkpoint\",\n",
        "    hub_model_id=\"crossdelenna/whisper_med_alex.en\",\n",
        "    hub_token='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',\n",
        "    resume_from_checkpoint=True  # This will resume training from the last checkpoint\n",
        ")\n",
        "\n",
        "# trainer = Seq2SeqTrainer(\n",
        "#     model=model,\n",
        "#     data_collator=data_collator,\n",
        "#     args=training_args,\n",
        "#     compute_metrics=compute_metrics,\n",
        "#     train_dataset=timit_train,\n",
        "#     eval_dataset=timit_test,\n",
        "#     tokenizer=processor.feature_extractor,\n",
        "# )\n"
      ],
      "metadata": {
        "id": "wIznLJGgsOpF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "838947b1-e78b-4792-81f3-cda229b6487b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1576: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-13-f11a85eabdf5>:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Seq2SeqTrainer to use sampled validation subset. Default random sample size is 300 from test data for faster evaluation at each eval_steps.\n"
      ],
      "metadata": {
        "id": "CBD4etzmSJGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to sample a subset of the validation data\n",
        "def sample_validation_data(dataset, sample_size=300, seed=42):\n",
        "    return dataset.shuffle(seed=seed).select(range(sample_size))\n",
        "\n",
        "class CustomSeq2SeqTrainer(Seq2SeqTrainer):\n",
        "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
        "        # Sample a smaller validation subset if it's an evaluation step\n",
        "        if self.state.global_step % self.args.eval_steps == 0:\n",
        "            eval_dataset = sample_validation_data(self.eval_dataset, sample_size=300)\n",
        "        else:\n",
        "            eval_dataset = self.eval_dataset\n",
        "        # Call the parent class's evaluate method with the modified eval_dataset\n",
        "        return super().evaluate(eval_dataset=eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
        "\n",
        "trainer = CustomSeq2SeqTrainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=timit_train,\n",
        "    eval_dataset=timit_test,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")\n"
      ],
      "metadata": {
        "id": "EM-0-P1_R-Dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save processor and tokenizer locally\n"
      ],
      "metadata": {
        "id": "a2OrtXhDsQ1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processor.save_pretrained(training_args.output_dir)\n",
        "tokenizer.save_pretrained(training_args.output_dir)\n"
      ],
      "metadata": {
        "id": "RM2awF4KsTS9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f80058e-076b-40ff-e8ea-7b31d8017c61"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./whisper-medium.en/tokenizer_config.json',\n",
              " './whisper-medium.en/special_tokens_map.json',\n",
              " './whisper-medium.en/vocab.json',\n",
              " './whisper-medium.en/merges.txt',\n",
              " './whisper-medium.en/normalizer.json',\n",
              " './whisper-medium.en/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model\n"
      ],
      "metadata": {
        "id": "RbeXAXXqsWMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"./whisper_med_alex.en/last-checkpoint\"  # Specify the path to the checkpoint\n",
        "\n",
        "trainer.train(resume_from_checkpoint=checkpoint_path)\n"
      ],
      "metadata": {
        "id": "jxAifpyNsZhH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "58a8041e-d494-40f5-a976-2811374471a7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='901' max='901' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [901/901 2:18:02, Epoch 3/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.209400</td>\n",
              "      <td>0.157849</td>\n",
              "      <td>10.239179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>0.129385</td>\n",
              "      <td>7.854614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.124400</td>\n",
              "      <td>0.120753</td>\n",
              "      <td>7.276537</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=901, training_loss=0.16121626560153496, metrics={'train_runtime': 8291.1482, 'train_samples_per_second': 2.608, 'train_steps_per_second': 0.109, 'total_flos': 2.20144478552064e+19, 'train_loss': 0.16121626560153496, 'epoch': 3.4787644787644787})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push to hub\n"
      ],
      "metadata": {
        "id": "gF8qXTNssbUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()\n"
      ],
      "metadata": {
        "id": "S6FDczc1sd7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "afe29f03f21a468c9de93db821df7119",
            "edad89c98e7049d1a10bf0ac21554a2e",
            "d4852d7defb346048f0cd37868e0f80e",
            "aff3eef26db848b3ab7cf7e2cf114f80",
            "aef5a276f519430bbeed5bba93db573f",
            "6df33a955217448794e957594804a244",
            "92574db1765742c0a8ca2d3b938089a9",
            "b0b2ae3bac694c57b41efb30fcaf771b",
            "18b6427a7b484e65aca93ec603484632",
            "fde9756055c64286b6bbede5bcf427bb",
            "32eb2c8bb97f44cf99baa29b47b92b0d"
          ]
        },
        "outputId": "cc37c4ed-e133-44e2-a316-878c6103a513"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "events.out.tfevents.1739240955.4cee2e63b939.6105.0:   0%|          | 0.00/7.85k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afe29f03f21a468c9de93db821df7119"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/crossdelenna/whisper_med_alex.en/commit/0891c81f4f4bd8599dc2ea3ea4d84146666136b1', commit_message='End of training', commit_description='', oid='0891c81f4f4bd8599dc2ea3ea4d84146666136b1', pr_url=None, repo_url=RepoUrl('https://huggingface.co/crossdelenna/whisper_med_alex.en', endpoint='https://huggingface.co', repo_type='model', repo_id='crossdelenna/whisper_med_alex.en'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save model, processor, and tokenizer locally\n"
      ],
      "metadata": {
        "id": "AJ_t0wjfsgtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processor.save_pretrained(training_args.output_dir)\n",
        "tokenizer.save_pretrained(training_args.output_dir)\n",
        "feature_extractor.save_pretrained(training_args.output_dir)\n"
      ],
      "metadata": {
        "id": "T-t9yD6MskMf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdc5d83a-99e7-4207-86b3-0e961aab7360"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./whisper-medium.en/preprocessor_config.json']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push processor and tokenizer to the Hugging Face Hub\n"
      ],
      "metadata": {
        "id": "AK77oX0-smX8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5R-Iww9FZ6Di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BBH0UhodqYGC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "aa586d36-27e0-49e0-c2bb-c8f9ce7f725b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/crossdelenna/medium_cross.en/commit/f4ca35fcba58dd44c960387b1f732188f7380c8a', commit_message='Upload feature extractor', commit_description='', oid='f4ca35fcba58dd44c960387b1f732188f7380c8a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/crossdelenna/medium_cross.en', endpoint='https://huggingface.co', repo_type='model', repo_id='crossdelenna/medium_cross.en'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "processor.push_to_hub(\"crossdelenna/whisper_med_alex.en\", token=\"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\", commit_message=\"Upload processor\")\n",
        "tokenizer.push_to_hub(\"crossdelenna/whisper_med_alex.en\", token=\"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\", commit_message=\"Upload tokenizer\")\n",
        "feature_extractor.push_to_hub(\"crossdelenna/medium_cross.en\", token=\"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\", commit_message=\"Upload feature extractor\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the tokenizers library\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# Load the existing components\n",
        "vocab_file = \"/content/whisper-medium.en/vocab.json\"\n",
        "merges_file = \"/content/whisper-medium.en/merges.txt\"\n",
        "tokenizer_config_file = \"/content/whisper-medium.en/tokenizer_config.json\"\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer(BPE(vocab_file=vocab_file, merges_file=merges_file))\n",
        "\n",
        "# Set pre-tokenizer and post-processor\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", 1),\n",
        "        (\"[SEP]\", 2),\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Save the tokenizer in the tokenizer.json format\n",
        "tokenizer.save(\"/content/whisper-medium.en/tokenizer.json\")\n",
        "\n",
        "# Verify that the tokenizer.json file has been created\n",
        "import os\n",
        "print(\"Files in the output directory:\", os.listdir(\"/content/whisper-medium.en\"))"
      ],
      "metadata": {
        "id": "l2mw7kNcZ8xG",
        "outputId": "418368d7-a2cc-4519-dc06-dc9e67718679",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.1.31)\n",
            "Files in the output directory: ['runs', 'checkpoint-600', 'README.md', 'checkpoint-900', 'checkpoint-901', 'special_tokens_map.json', 'checkpoint-300', 'training_args.bin', 'added_tokens.json', 'normalizer.json', 'tokenizer_config.json', 'tokenizer.json', 'preprocessor_config.json', 'generation_config.json', 'config.json', 'vocab.json', 'merges.txt', 'model.safetensors']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the huggingface_hub library\n",
        "\n",
        "from huggingface_hub import HfApi, HfFolder\n",
        "\n",
        "# Define your repository and token\n",
        "repo_id = \"crossdelenna/whisper_med_alex.en\"\n",
        "token = \"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\"\n",
        "\n",
        "# Initialize the HfApi\n",
        "api = HfApi()\n",
        "\n",
        "# Upload the tokenizer.json file\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"./whisper-medium.en/tokenizer.json\",\n",
        "    path_in_repo=\"tokenizer.json\",\n",
        "    repo_id=repo_id,\n",
        "    token=token,\n",
        "    commit_message=\"Upload tokenizer.json\"\n",
        ")\n",
        "\n",
        "# Verify that the file has been uploaded\n",
        "print(f\"Uploaded tokenizer.json to https://huggingface.co/{repo_id}/blob/main/tokenizer.json\")"
      ],
      "metadata": {
        "id": "-u6QCMImaOAI",
        "outputId": "0705a626-ad8d-475e-9a68-69fd2721e65e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
            "Uploaded tokenizer.json to https://huggingface.co/crossdelenna/whisper_med_alex.en/blob/main/tokenizer.json\n"
          ]
        }
      ]
    }
  ]
}