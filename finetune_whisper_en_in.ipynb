{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quick_finetune_large-en-in-lm.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beinghorizontal/wav2vec2/blob/main/finetune_whisper_en_in.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "thdGplBzw5Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "ZgsoCDhxL8wX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import datasets\n",
        "!pip install datasets\n",
        "import datasets\n",
        "timit = datasets.load_dataset(\"crossdelenna/whisper_data_merge\", use_auth_token='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')"
      ],
      "metadata": {
        "id": "aqmpHpyDIlPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_rows = int(len(timit['train']))\n",
        "num_test_rows = int(len(timit['train'])/7)\n",
        "num_train_rows = num_rows - num_test_rows\n",
        "timit_train = timit[\"train\"].shuffle(seed=42).select(range(num_train_rows))\n",
        "timit_test = timit[\"train\"].shuffle(seed=42).select(range(num_test_rows))\n"
      ],
      "metadata": {
        "id": "dEBV5p-vIdx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timit_train"
      ],
      "metadata": {
        "id": "yZg6TeS2MH5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timit_test"
      ],
      "metadata": {
        "id": "EtgFoypLM2al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "#!unzip \"/content/drive/MyDrive/checkpoints_large.zip\" -d \"/content/\"\n",
        "#!unzip \"/content/drive/MyDrive/checkpoints_large.zip\" -d \"/content/\""
      ],
      "metadata": {
        "id": "fYAXooWRJJne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#!pip install datasets==1.18.3\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install jiwer\n",
        "!pip install librosa\n",
        "!pip install evaluate>=0.30\n",
        "!pip install jiwer\n",
        "!pip install gradio\n",
        "!pip install bitsandbytes\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9QLXpnFTJdTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "QVPX-Lkro1Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path to upload checkpoints to drive\n",
        "#from shutil import copyfile\n",
        "#dst = '/usr/local/lib/python3.9/dist-packages/transformers/trainer.py'\n",
        "#src = '/content/drive/MyDrive/trainer_mod_large.py'\n",
        "#copyfile(src, dst)"
      ],
      "metadata": {
        "id": "y7KsYbx1Jn4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!apt install git-lfs"
      ],
      "metadata": {
        "id": "kODiImxmJ7zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "display(IPython.display.Javascript('''\n",
        " function ClickConnect(){\n",
        "   btn = document.querySelector(\"colab-connect-button\")\n",
        "   if (btn != null){\n",
        "     console.log(\"Click colab-connect-button\"); \n",
        "     btn.click() \n",
        "     }\n",
        "   \n",
        "   btn = document.getElementById('ok')\n",
        "   if (btn != null){\n",
        "     console.log(\"Click reconnect\"); \n",
        "     btn.click() \n",
        "     }\n",
        "  }\n",
        "  \n",
        "setInterval(ClickConnect,60000)\n",
        "'''))\n",
        "\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "WuaRjtflJ-CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load WhisperFeatureExtractor\n"
      ],
      "metadata": {
        "id": "PFSffHApkAVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small.en\")\n",
        "#feature_extractor = WhisperFeatureExtractor.from_pretrained(\"crossdelenna/whisper-small.en\")\n"
      ],
      "metadata": {
        "id": "DIELrbALjgG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load WhisperTokenizer"
      ],
      "metadata": {
        "id": "stPDLMChkKzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperTokenizer\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small.en\", language=\"English\", task=\"transcribe\")\n",
        "#tokenizer = WhisperTokenizer.from_pretrained(\"crossdelenna/whisper-small.en\", language=\"English\", task=\"transcribe\")"
      ],
      "metadata": {
        "id": "NjMiBtavkM9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine To Create A WhisperProcessor"
      ],
      "metadata": {
        "id": "MnfY-n9PlOsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small.en\", language=\"English\", task=\"transcribe\")\n",
        "#processor = WhisperProcessor.from_pretrained(\"crossdelenna/whisper-small.en\", language=\"English\", task=\"transcribe\")"
      ],
      "metadata": {
        "id": "mrdVc2y3lPvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print the first example of the Common Voice dataset to see \n",
        "what form the data is in:"
      ],
      "metadata": {
        "id": "eMbijzjalj_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(timit[\"train\"][0])"
      ],
      "metadata": {
        "id": "UmqNXTPnlniJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKcEWHvKI1by"
      },
      "source": [
        "**Note**: Currently `datasets` make use of [`torchaudio`](https://pytorch.org/audio/stable/index.html) and [`librosa`](https://librosa.org/doc/latest/index.html) for audio loading and resampling. If you wish to implement your own costumized data loading/sampling, feel free to just make use of the `\"path\"` column instead and disregard the `\"audio\"` column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tborvC9hx88e"
      },
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbQf5GuZyQ4_"
      },
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO-Zdj-5cxXp"
      },
      "source": [
        "Next, the evaluation metric is defined. As mentioned earlier, the \n",
        "predominant metric in ASR is the word error rate (WER), hence we will use it in this notebook as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xsux2gmyXso"
      },
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XZ-kjweyTy_"
      },
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# #dics = os.listdir('/content/drive/MyDrive/wav2vec2-base-en-in/wav2vec2-base-en-in')\n",
        "# dics = os.listdir('/content/wav2vec2-large-eng-ind')\n",
        "# if dics[0] == '.ipynb_checkpoints':\n",
        "#   dics=dics[1:]\n",
        "# match = [t for t in dics if 'checkpoint' in t]\n",
        "# model_url = '/content/wav2vec2-large-en-in/'+match[0]\n",
        "# print(model_url)\n"
      ],
      "metadata": {
        "id": "cNjNj9wz7rzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7cqAWIayn6w"
      },
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "#model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small.en\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"crossdelenna/whisper-small.en\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ],
      "metadata": {
        "id": "Ls9zaYSQnwTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-small.en\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=10,\n",
        "    max_steps=901,  # default is 4000 I changed to 2k so it stops training early and export model to HF repo\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=16,  #was working with 1 but very slow\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=300,\n",
        "    eval_steps=300,\n",
        "    logging_steps=300,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        "    hub_token = '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',\n",
        "    #optim=\"adamw_bnb_8bit\",\n",
        ")"
      ],
      "metadata": {
        "id": "-RhXA3HG0UoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsW-WZcL1ZtN"
      },
      "source": [
        "Now, all instances can be passed to Trainer and we are ready to start training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY7vBmFCPFgC"
      },
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=timit_train,\n",
        "    eval_dataset=timit_test,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor.save_pretrained(training_args.output_dir)"
      ],
      "metadata": {
        "id": "COaGLAo6qyj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpvZHM1xReIW"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYYAvgkW4P0m"
      },
      "source": [
        "```javascript\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\"); \n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UEjJqGsQw24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "outputId": "9b48f2cc-c51a-4696-afc9-c36326f4e6ab"
      },
      "source": [
        "#trainer.train(resume_from_checkpoint=True)\n",
        "trainer.train()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1112' max='1201' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1112/1201 2:19:50 < 11:12, 0.13 it/s, Epoch 1.99/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.145800</td>\n",
              "      <td>0.104029</td>\n",
              "      <td>4.833385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.143900</td>\n",
              "      <td>0.064490</td>\n",
              "      <td>3.409677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.066400</td>\n",
              "      <td>0.046952</td>\n",
              "      <td>2.408955</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "3O73H-aAwdDZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}