{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quick_finetune_large-en-in-lm.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beinghorizontal/wav2vec2/blob/main/finetune_whisper_medium_in.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "thdGplBzw5Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "ZgsoCDhxL8wX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import datasets\n",
        "#locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install datasets\n",
        "import datasets\n",
        "timit = datasets.load_dataset(\"crossdelenna/whisper_data_merge2\", use_auth_token='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')"
      ],
      "metadata": {
        "id": "aqmpHpyDIlPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_rows = int(len(timit['train']))\n",
        "num_test_rows = int(len(timit['train'])/7)\n",
        "num_train_rows = num_rows - num_test_rows\n",
        "timit_train = timit[\"train\"].shuffle(seed=42).select(range(num_train_rows))\n",
        "timit_test = timit[\"train\"].shuffle(seed=42).select(range(num_test_rows))\n"
      ],
      "metadata": {
        "id": "dEBV5p-vIdx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timit_train"
      ],
      "metadata": {
        "id": "yZg6TeS2MH5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timit_test"
      ],
      "metadata": {
        "id": "EtgFoypLM2al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import locale\n",
        "#locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!unzip \"/content/drive/MyDrive/mycheckpoint.zip\" -d \"/content/\"\n"
      ],
      "metadata": {
        "id": "fYAXooWRJJne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#!pip install datasets==1.18.3\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install jiwer\n",
        "!pip install librosa\n",
        "!pip install evaluate>=0.30\n",
        "!pip install jiwer\n",
        "!pip install gradio\n",
        "!pip install bitsandbytes\n",
        "!pip install accelerate\n",
        "\n",
        "\n",
        "from accelerate import PartialState\n"
      ],
      "metadata": {
        "id": "9QLXpnFTJdTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "QVPX-Lkro1Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path to upload checkpoints to drive\n",
        "#from shutil import copyfile\n",
        "#dst = '/usr/local/lib/python3.9/dist-packages/transformers/trainer.py'\n",
        "#src = '/content/drive/MyDrive/trainer_mod_large.py'\n",
        "#copyfile(src, dst)"
      ],
      "metadata": {
        "id": "y7KsYbx1Jn4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!apt install git-lfs"
      ],
      "metadata": {
        "id": "kODiImxmJ7zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "display(IPython.display.Javascript('''\n",
        " function ClickConnect(){\n",
        "   btn = document.querySelector(\"colab-connect-button\")\n",
        "   if (btn != null){\n",
        "     console.log(\"Click colab-connect-button\");\n",
        "     btn.click()\n",
        "     }\n",
        "\n",
        "   btn = document.getElementById('ok')\n",
        "   if (btn != null){\n",
        "     console.log(\"Click reconnect\");\n",
        "     btn.click()\n",
        "     }\n",
        "  }\n",
        "\n",
        "setInterval(ClickConnect,60000)\n",
        "'''))\n",
        "\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "WuaRjtflJ-CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load WhisperFeatureExtractor\n"
      ],
      "metadata": {
        "id": "PFSffHApkAVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-medium.en\")\n",
        "#feature_extractor = WhisperFeatureExtractor.from_pretrained(\"crossdelenna/whisper-small.en\")\n"
      ],
      "metadata": {
        "id": "DIELrbALjgG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load WhisperTokenizer"
      ],
      "metadata": {
        "id": "stPDLMChkKzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperTokenizer\n",
        "#tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small.en\", language=\"English\", task=\"transcribe\")\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"crossdelenna/whisper-medium.en\", language=\"English\", task=\"transcribe\")"
      ],
      "metadata": {
        "id": "NjMiBtavkM9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine To Create A WhisperProcessor"
      ],
      "metadata": {
        "id": "MnfY-n9PlOsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium.en\", language=\"English\", task=\"transcribe\")\n",
        "#processor = WhisperProcessor.from_pretrained(\"crossdelenna/whisper-small.en\", language=\"English\", task=\"transcribe\")"
      ],
      "metadata": {
        "id": "mrdVc2y3lPvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print the first example of the Common Voice dataset to see\n",
        "what form the data is in:"
      ],
      "metadata": {
        "id": "eMbijzjalj_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(timit[\"train\"][0])"
      ],
      "metadata": {
        "id": "UmqNXTPnlniJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKcEWHvKI1by"
      },
      "source": [
        "**Note**: Currently `datasets` make use of [`torchaudio`](https://pytorch.org/audio/stable/index.html) and [`librosa`](https://librosa.org/doc/latest/index.html) for audio loading and resampling. If you wish to implement your own costumized data loading/sampling, feel free to just make use of the `\"path\"` column instead and disregard the `\"audio\"` column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tborvC9hx88e"
      },
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbQf5GuZyQ4_"
      },
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO-Zdj-5cxXp"
      },
      "source": [
        "Next, the evaluation metric is defined. As mentioned earlier, the\n",
        "predominant metric in ASR is the word error rate (WER), hence we will use it in this notebook as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xsux2gmyXso"
      },
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XZ-kjweyTy_"
      },
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# #dics = os.listdir('/content/drive/MyDrive/wav2vec2-base-en-in/wav2vec2-base-en-in')\n",
        "# dics = os.listdir('/content/wav2vec2-large-eng-ind')\n",
        "# if dics[0] == '.ipynb_checkpoints':\n",
        "#   dics=dics[1:]\n",
        "# match = [t for t in dics if 'checkpoint' in t]\n",
        "# model_url = '/content/wav2vec2-large-en-in/'+match[0]\n",
        "# print(model_url)\n"
      ],
      "metadata": {
        "id": "cNjNj9wz7rzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7cqAWIayn6w"
      },
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium.en\")\n",
        "# model = WhisperForConditionalGeneration.from_pretrained(\"crossdelenna/whisper-small.en\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ],
      "metadata": {
        "id": "Ls9zaYSQnwTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get the list of files and directories in the \"/content/whisper-small.en\" folder.\n",
        "files_and_directories = os.listdir(\"/content/whisper-small.en\")\n",
        "\n",
        "# Filter the list to only include directories.\n",
        "directories = [f for f in files_and_directories if os.path.isdir(os.path.join(\"/content/whisper-small.en\", f))]\n",
        "\n",
        "# Find the directory that starts with \"checkpoint-1200\".\n",
        "checkpoint_directory = next((d for d in directories if d.startswith(\"checkpoint-1200\")), None)\n",
        "\n",
        "# Print the checkpoint directory name.\n",
        "if checkpoint_directory:\n",
        "  print(checkpoint_directory)\n",
        "else:\n",
        "  print(\"Checkpoint directory not found.\")"
      ],
      "metadata": {
        "id": "5_v0x40QpCBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: huggingface transformers resume from checkpoint give specific path of checkpoint\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "maxsteps = int(checkpoint_directory.split('-')[1])+1201\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-medium.en\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=10,\n",
        "    max_steps=maxsteps,  # default is 4000 I changed to 2k so it stops training early and export model to HF repo\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,  #was working with 1 but very slow\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=400,\n",
        "    eval_steps=400,\n",
        "    logging_steps=400,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"crossdelenna/medium_cross.en\",\n",
        "    hub_token = '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',\n",
        "    #optim=\"adamw_bnb_8bit\",\n",
        ")\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=timit_train,\n",
        "    eval_dataset=timit_test,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Ek0TgpuxPgOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsW-WZcL1ZtN"
      },
      "source": [
        "Now, all instances can be passed to Trainer and we are ready to start training!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor.save_pretrained(training_args.output_dir)"
      ],
      "metadata": {
        "id": "COaGLAo6qyj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpvZHM1xReIW"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYYAvgkW4P0m"
      },
      "source": [
        "```javascript\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\");\n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resume training from a specific checkpoint\n",
        "#resume_from_checkpoint = f\"/content/whisper-small.en/checkpoint-{maxsteps-1}\"\n",
        "checkpointpath = f'/content/whisper-small.en/{checkpoint_directory}'\n",
        "#checkpointpath = \"/content/content/whisper-small.en/checkpoint-800\"\n",
        "resume_from_checkpoint = checkpointpath\n"
      ],
      "metadata": {
        "id": "DIVKJ6LeP69F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: huggingface how to resume training from checkpoint with path\n",
        "\n",
        "trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n"
      ],
      "metadata": {
        "id": "m46NP6OjQNlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UEjJqGsQw24"
      },
      "source": [
        "#trainer.train(resume_from_checkpoint=True)\n",
        "#trainer.train()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "3O73H-aAwdDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!zip -r /content/whisper-small.en/mycheckpoint.zip /content/whisper-small.en/{checkpoint_directory}/\n",
        "!mv /content/whisper-small.en/mycheckpoint.zip /content/drive/MyDrive/\n"
      ],
      "metadata": {
        "id": "8fq9W5Yd-W0n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}