{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beinghorizontal/wav2vec2/blob/main/whisper_turbo_peft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cefac89"
      },
      "source": [
        "# Fine-tune Whisper (large) with LoRA & BNB powerd by PEFT ‚ö°Ô∏è"
      ],
      "id": "5cefac89"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "625e47a0"
      },
      "source": [
        "## Prepare Environment\n",
        "\n",
        "We'll employ several popular Python packages to fine-tune the Whisper model.\n",
        "We'll use `datasets` to download and prepare our training data and\n",
        "`transformers` to load and train our Whisper model. We'll also require\n",
        "the `librosa` package to pre-process audio files, `evaluate` and `jiwer` to\n",
        "assess the performance of our model. Finally, we'll\n",
        "use `PEFT`, `bitsandbytes`, `accelerate` to prepare and fine-tune the model with LoRA."
      ],
      "id": "625e47a0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_Ivl7qlX0dz"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets librosa evaluate jiwer gradio bitsandbytes accelerate\n",
        "!pip install -q git+https://github.com/huggingface/peft.git@main"
      ],
      "id": "r_Ivl7qlX0dz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MvOaXjUjK71"
      },
      "source": [
        "With the environment now set up, let's try to secure a decent GPU for our Colab! Unfortunately, it's becoming much harder to get access to a good GPU with the free version of Google Colab. However, with Google Colab Pro one should have no issues in being allocated a V100 or P100 GPU.\n",
        "\n",
        "To get a GPU, click _Runtime_ -> _Change runtime type_, then change _Hardware accelerator_ from _None_ to _GPU_.\n",
        "\n",
        "We can verify that we've been assigned a GPU and view its specifications:"
      ],
      "id": "2MvOaXjUjK71"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kBtM9XSjKE5"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "id": "2kBtM9XSjKE5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WwnavbBuezQ"
      },
      "source": [
        "Alrighty! Let's configure our environment to ensure it uses the GPU provided by Colab to us."
      ],
      "id": "6WwnavbBuezQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!unzip \"/content/drive/MyDrive/mycheckpoint.zip\" -d \"/content/\"\n"
      ],
      "metadata": {
        "id": "VonALyYAA4sH"
      },
      "id": "VonALyYAA4sH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1da5fff"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "id": "e1da5fff"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed0OpduhX2JF"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "id": "ed0OpduhX2JF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG-2lYDPw3uW"
      },
      "source": [
        "Next up, we define Whisper model checkpoints and task details."
      ],
      "id": "gG-2lYDPw3uW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJ9M1WKhu0KM"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"openai/whisper-large-v3-turbo\"\n",
        "task = \"transcribe\""
      ],
      "id": "mJ9M1WKhu0KM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuhppXc9xAt2"
      },
      "source": [
        "Lastly, we define the dataset details, including the language we'd like to fine-tune Whisper on too."
      ],
      "id": "EuhppXc9xAt2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sE0FPf7w-he"
      },
      "outputs": [],
      "source": [
        "language = \"English\"\n",
        "language_abbr = \"en\" # Short hand code for the language we want to fine-tune"
      ],
      "id": "7sE0FPf7w-he"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2787582-554f-44ce-9f38-4180a5ed6b44"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "timit = load_dataset(\"crossdelenna/whisper_data-turbo\")\n"
      ],
      "id": "a2787582-554f-44ce-9f38-4180a5ed6b44"
    },
    {
      "cell_type": "code",
      "source": [
        "num_rows = int(len(timit['train']))\n",
        "num_test_rows = int(len(timit['train'])/3)\n",
        "num_train_rows = num_rows - num_test_rows\n",
        "timit_train = timit[\"train\"].shuffle(seed=42).select(range(num_train_rows))\n",
        "timit_test = timit[\"train\"].shuffle(seed=42).select(range(num_test_rows))\n"
      ],
      "metadata": {
        "id": "dEBV5p-vIdx-"
      },
      "execution_count": null,
      "outputs": [],
      "id": "dEBV5p-vIdx-"
    },
    {
      "cell_type": "code",
      "source": [
        "timit_train"
      ],
      "metadata": {
        "id": "I4bJzf2fktTJ"
      },
      "id": "I4bJzf2fktTJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timit_test"
      ],
      "metadata": {
        "id": "WEI_22EkkuC6"
      },
      "id": "WEI_22EkkuC6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timit_train[0]"
      ],
      "metadata": {
        "id": "zkezVN3UqZNm"
      },
      "id": "zkezVN3UqZNm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d63b2d2-f68a-4d74-b7f1-5127f6d16605"
      },
      "source": [
        "## Prepare Feature Extractor, Tokenizer and Data\n",
        "\n",
        "The ASR pipeline can be de-composed into three stages:\n",
        "1. A feature extractor which pre-processes the raw audio-inputs\n",
        "2. The model which performs the sequence-to-sequence mapping\n",
        "3. A tokenizer which post-processes the model outputs to text format\n",
        "\n",
        "In ü§ó Transformers, the Whisper model has an associated feature extractor and tokenizer,\n",
        "called [WhisperFeatureExtractor](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor)\n",
        "and [WhisperTokenizer](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer)\n",
        "respectively."
      ],
      "id": "2d63b2d2-f68a-4d74-b7f1-5127f6d16605"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import Repository\n",
        "\n",
        "repo_name = \"crossdelenna/whisperturbo\"\n",
        "local_dir = \"./whisperturbo\"\n",
        "\n",
        "# Clone the repository\n",
        "repo = Repository(local_dir=local_dir, clone_from=repo_name)"
      ],
      "metadata": {
        "id": "6Tc_fqI9MYYj"
      },
      "id": "6Tc_fqI9MYYj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_local_path = os.path.join(local_dir, \"checkpoints\", \"checkpoint-xxx\")\n"
      ],
      "metadata": {
        "id": "OsglmFiBM5IB"
      },
      "id": "OsglmFiBM5IB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7b07f9b-ae0e-4f89-98f0-0c50d432eab6"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)"
      ],
      "id": "c7b07f9b-ae0e-4f89-98f0-0c50d432eab6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIaGxWbXkcrC"
      },
      "source": [
        "To simplify using the feature extractor and tokenizer, we can _wrap_ both into a single `WhisperProcessor` class. This processor object can be used on the audio inputs and model predictions as required.\n",
        "In doing so, we only need to keep track of two objects during training:\n",
        "the `processor` and the `model`:"
      ],
      "id": "gIaGxWbXkcrC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77d9f0c5-8607-4642-a8ac-c3ab2e223ea6"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
      ],
      "id": "77d9f0c5-8607-4642-a8ac-c3ab2e223ea6"
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the first training sample\n",
        "sample = timit[\"train\"][0][\"input_features\"]\n",
        "\n",
        "# Convert to torch tensor and process with the feature extractor\n",
        "import torch\n",
        "input_tensor = torch.tensor(sample)  # Ensure the input is a tensor\n",
        "\n",
        "# Verify the input tensor shape\n",
        "print(\"Shape of raw input_features:\", input_tensor.shape)\n",
        "\n",
        "# If you want to see how the feature extractor processes it (assuming it takes raw audio):\n",
        "# processed_features = feature_extractor(input_tensor, sampling_rate=16000, return_tensors=\"pt\")\n",
        "# print(\"Shape of processed input features:\", processed_features[\"input_features\"].shape)\n"
      ],
      "metadata": {
        "id": "L7ZglJOBUV-r"
      },
      "execution_count": null,
      "outputs": [],
      "id": "L7ZglJOBUV-r"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d230e6d-624c-400a-bbf5-fa660881df25"
      },
      "source": [
        "### Define a Data Collator\n",
        "\n",
        "The data collator for a sequence-to-sequence speech model is unique in the sense that it\n",
        "treats the `input_features` and `labels` independently: the  `input_features` must be\n",
        "handled by the feature extractor and the `labels` by the tokenizer.\n",
        "\n",
        "The `input_features` are already padded to 30s and converted to a log-Mel spectrogram\n",
        "of fixed dimension by action of the feature extractor, so all we have to do is convert the `input_features`\n",
        "to batched PyTorch tensors. We do this using the feature extractor's `.pad` method with `return_tensors=pt`.\n",
        "\n",
        "The `labels` on the other hand are un-padded. We first pad the sequences\n",
        "to the maximum length in the batch using the tokenizer's `.pad` method. The padding tokens\n",
        "are then replaced by `-100` so that these tokens are **not** taken into account when\n",
        "computing the loss. We then cut the BOS token from the start of the label sequence as we\n",
        "append it later during training.\n",
        "\n",
        "We can leverage the `WhisperProcessor` we defined earlier to perform both the\n",
        "feature extractor and the tokenizer operations:"
      ],
      "id": "8d230e6d-624c-400a-bbf5-fa660881df25"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cae7dbf-8a50-456e-a3a8-7fd005390f86"
      },
      "source": [
        "Let's initialise the data collator we've just defined:"
      ],
      "id": "3cae7dbf-8a50-456e-a3a8-7fd005390f86"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc834702-c0d3-4a96-b101-7b87be32bf42"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ],
      "id": "fc834702-c0d3-4a96-b101-7b87be32bf42"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d62bb2ab-750a-45e7-82e9-61d6f4805698"
      },
      "source": [
        "### Evaluation Metrics"
      ],
      "id": "d62bb2ab-750a-45e7-82e9-61d6f4805698"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66fee1a7-a44c-461e-b047-c3917221572e"
      },
      "source": [
        "We'll use the word error rate (WER) metric, the 'de-facto' metric for assessing\n",
        "ASR systems. For more information, refer to the WER [docs](https://huggingface.co/metrics/wer). We'll load the WER metric from ü§ó Evaluate:"
      ],
      "id": "66fee1a7-a44c-461e-b047-c3917221572e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b22b4011-f31f-4b57-b684-c52332f92890"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ],
      "id": "b22b4011-f31f-4b57-b684-c52332f92890"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daf2a825-6d9f-4a23-b145-c37c0039075b"
      },
      "source": [
        "###¬†Load a Pre-Trained Checkpoint"
      ],
      "id": "daf2a825-6d9f-4a23-b145-c37c0039075b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "437a97fa-4864-476b-8abc-f28b8166cfa5"
      },
      "source": [
        "Now let's load the pre-trained Whisper checkpoint. Again, this\n",
        "is trivial through use of ü§ó Transformers!\n",
        "\n",
        "To reduce our models memory footprint, we load the model in 8bit, this means we quantize the model to use 1/4th precision (when comapared to float32) with minimal loss to performance. To read more about how this works, head over [here](https://huggingface.co/blog/hf-bitsandbytes-integration)."
      ],
      "id": "437a97fa-4864-476b-8abc-f28b8166cfa5"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "xFdvROfBsPEa"
      },
      "id": "xFdvROfBsPEa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a10cc4b-07ec-4ebd-ac1d-7c601023594f"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, load_in_8bit=True,device_map=\"auto\")"
      ],
      "id": "5a10cc4b-07ec-4ebd-ac1d-7c601023594f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR-_yaEOPsfQ"
      },
      "source": [
        "### Post-processing on the model\n",
        "\n",
        "Finally, we need to apply some post-processing steps on the 8-bit model to enable training. We do so by first freezing all the model layers, and then cast the layer-norm and the output layer in `float32` for training and model stability."
      ],
      "id": "bR-_yaEOPsfQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "# Load the LoRA configuration\n",
        "peft_config = PeftConfig.from_pretrained(os.path.join(local_dir, \"checkpoint-800\"))\n",
        "\n",
        "# Load the LoRA adapter\n",
        "model = PeftModel.from_pretrained(model, os.path.join(local_dir, \"checkpoint-800\"))"
      ],
      "metadata": {
        "id": "QlYc2WE2fGjY"
      },
      "id": "QlYc2WE2fGjY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Load training arguments\n",
        "#training_args = TrainingArguments.from_pretrained(local_dir)\n",
        "training_args = torch.load(\"/content/whisperturbo/training_args.bin\")\n"
      ],
      "metadata": {
        "id": "ZcstSkJtjBsu"
      },
      "id": "ZcstSkJtjBsu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cl_ZQualPt9R"
      },
      "outputs": [],
      "source": [
        "#from peft import prepare_model_for_kbit_training\n",
        "\n",
        "#model = prepare_model_for_kbit_training(model)"
      ],
      "id": "Cl_ZQualPt9R"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the Whisper model uses Convolutional layers in the Encoder, checkpointing disables grad computation to avoid this we specifically need to make the inputs trainable."
      ],
      "metadata": {
        "id": "p0Ja2e__OX02"
      },
      "id": "p0Ja2e__OX02"
    },
    {
      "cell_type": "code",
      "source": [
        "def make_inputs_require_grad(module, input, output):\n",
        "    output.requires_grad_(True)\n",
        "\n",
        "model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)"
      ],
      "metadata": {
        "id": "bmpeiajSOWCy"
      },
      "id": "bmpeiajSOWCy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjl4j4RJPmPR"
      },
      "source": [
        "### Apply Low-rank adapters (LoRA) to the model\n",
        "\n",
        "Here comes the magic with `peft`! Let's load a `PeftModel` and specify that we are going to use low-rank adapters (LoRA) using `get_peft_model` utility function from `peft`."
      ],
      "id": "Vjl4j4RJPmPR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQtpDPRHPyOL"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "#model = get_peft_model(model, crossdelenna/whisperturbo)\n",
        "model.print_trainable_parameters()"
      ],
      "id": "DQtpDPRHPyOL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3906d436"
      },
      "source": [
        "We are ONLY using **1%** of the total trainable parameters, thereby performing **Parameter-Efficient Fine-Tuning**"
      ],
      "id": "3906d436"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2178dea4-80ca-47b6-b6ea-ba1915c90c06"
      },
      "source": [
        "### Define the Training Configuration"
      ],
      "id": "2178dea4-80ca-47b6-b6ea-ba1915c90c06"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c21af1e9-0188-4134-ac82-defc7bdcc436"
      },
      "source": [
        "In the final step, we define all the parameters related to training. For more detail on the training arguments, refer to the Seq2SeqTrainingArguments [docs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)."
      ],
      "id": "c21af1e9-0188-4134-ac82-defc7bdcc436"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"whisperturbo\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-3,\n",
        "    warmup_steps=50,\n",
        "    num_train_epochs=1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    fp16=True,\n",
        "    per_device_eval_batch_size=8,\n",
        "    generation_max_length=128,\n",
        "    save_steps=400,\n",
        "    eval_steps=400,\n",
        "    logging_steps=400,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    gradient_checkpointing=True,\n",
        "    push_to_hub=True,\n",
        "    hub_token = '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',\n",
        "    max_steps=1200, # only for testing purposes, remove this from your final run :)\n",
        "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
        "    label_names=[\"labels\"],  # same reason as above\n",
        ")"
      ],
      "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3a944d8-3112-4552-82a0-be25988b3857"
      },
      "source": [
        "Fine-tuning a model with PEFT comes with a few caveats.\n",
        "\n",
        "1. We need to explicitly set `remove_unused_columns=False` and `label_names=[\"labels\"]` as the PeftModel's forward doesn't inherit the signature of the base model's forward.\n",
        "\n",
        "2. Since INT8 training requires autocasting, we cannot use the native `predict_with_generate` call in Trainer as it doesn't automatically cast.\n",
        "\n",
        "3. Similarly, since we cannot autocast, we cannot pass the `compute_metrics` to `Seq2SeqTrainer` so we'll comment it out whilst instantiating the Trainer."
      ],
      "id": "b3a944d8-3112-4552-82a0-be25988b3857"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d546d7fe-0543-479a-b708-2ebabec19493"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
        "\n",
        "# This callback helps to save only the adapter weights and remove the base model weights.\n",
        "class SavePeftModelCallback(TrainerCallback):\n",
        "    def on_save(\n",
        "        self,\n",
        "        args: TrainingArguments,\n",
        "        state: TrainerState,\n",
        "        control: TrainerControl,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
        "\n",
        "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
        "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
        "\n",
        "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
        "        if os.path.exists(pytorch_model_path):\n",
        "            os.remove(pytorch_model_path)\n",
        "        return control\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=timit_train,\n",
        "    eval_dataset=timit_test,\n",
        "    data_collator=data_collator,\n",
        "    # compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    callbacks=[SavePeftModelCallback],\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ],
      "id": "d546d7fe-0543-479a-b708-2ebabec19493"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee8b7b8e-1c9a-4d77-9137-1778a629e6de"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(os.path.join('/content/whisperturbo', \"checkpoint-1\"))\n",
        "\n"
      ],
      "id": "ee8b7b8e-1c9a-4d77-9137-1778a629e6de"
    },
    {
      "cell_type": "code",
      "source": [
        "#trainer.train(resume_from_checkpoint =\"/content/whisperturbo\")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "RsRZvHh5WTyu"
      },
      "id": "RsRZvHh5WTyu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "2ZR7JwHJnpTX"
      },
      "id": "2ZR7JwHJnpTX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a zip file of whisperturbo folder\n",
        "\n",
        "!zip -r /content/whisperturbo/whisperturbo.zip whisperturbo\n"
      ],
      "metadata": {
        "id": "HdsYpHlsB_J1"
      },
      "id": "HdsYpHlsB_J1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "! zip -r /content/whisperturbo/mycheckpoint.zip\n",
        "!mv /content/whisperturbo/mycheckpoint.zip /content/drive/MyDrive/\n"
      ],
      "metadata": {
        "id": "pRYTJ_BcWTD7"
      },
      "id": "pRYTJ_BcWTD7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our model is fine-tuned, we can push the model on to Hugging Face Hub, this will later help us directly infer the model from the model repo."
      ],
      "metadata": {
        "id": "8iqXhUiuBQCs"
      },
      "id": "8iqXhUiuBQCs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlyyOGnPgi_I"
      },
      "source": [
        "# Evaluation and Inference"
      ],
      "id": "SlyyOGnPgi_I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kzfg2qoXgrhg"
      },
      "source": [
        "On to the fun part, we've successfully fine-tuned our model. Now let's put it to test and calculate the WER on the `test` set.\n",
        "\n",
        "As with training, we do have a few caveats to pay attention to:\n",
        "1. Since we cannot use `predict_with_generate` function, we will hand roll our own eval loop with `torch.cuda.amp.autocast()` you can check it out below.\n",
        "2. Since the base model is frozen, PEFT model sometimes fails to recognise the language while decoding. To fix that, we force the starting tokens to mention the language we are transcribing. This is done via `forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"Marathi\", task=\"transcribe\")` and passing that too the `model.generate` call.\n",
        "\n",
        "That's it, let's get transcribing! üî•\n"
      ],
      "id": "Kzfg2qoXgrhg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "273a996c"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer\n",
        "\n",
        "peft_model_id = \"reach-vb/whisper-large-v2-hindi-100steps\" # Use the same model ID as before.\n",
        "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\n",
        "    peft_config.base_model_name_or_path, load_in_8bit=True, device_map=\"auto\"\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "model.config.use_cache = True"
      ],
      "id": "273a996c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "401ceaa6"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
        "\n",
        "eval_dataloader = DataLoader(common_voice[\"test\"], batch_size=8, collate_fn=data_collator)\n",
        "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n",
        "normalizer = BasicTextNormalizer()\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "normalized_predictions = []\n",
        "normalized_references = []\n",
        "\n",
        "model.eval()\n",
        "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
        "    with torch.cuda.amp.autocast():\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = (\n",
        "                model.generate(\n",
        "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
        "                    forced_decoder_ids=forced_decoder_ids,\n",
        "                    max_new_tokens=255,\n",
        "                )\n",
        "                .cpu()\n",
        "                .numpy()\n",
        "            )\n",
        "            labels = batch[\"labels\"].cpu().numpy()\n",
        "            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
        "            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "            predictions.extend(decoded_preds)\n",
        "            references.extend(decoded_labels)\n",
        "            normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n",
        "            normalized_references.extend([normalizer(label).strip() for label in decoded_labels])\n",
        "        del generated_tokens, labels, batch\n",
        "    gc.collect()\n",
        "wer = 100 * metric.compute(predictions=predictions, references=references)\n",
        "normalized_wer = 100 * metric.compute(predictions=normalized_predictions, references=normalized_references)\n",
        "eval_metrics = {\"eval/wer\": wer, \"eval/normalized_wer\": normalized_wer}\n",
        "\n",
        "print(f\"{wer=} and {normalized_wer=}\")\n",
        "print(eval_metrics)"
      ],
      "id": "401ceaa6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fin!\n",
        "\n",
        "If you made it all the way till the end then pat yourself on the back. Looking back, we learned how to train *any* Whisper checkpoint faster, cheaper and with negligible loss in WER.\n",
        "\n",
        "With PEFT, you can also go beyond Speech recognition and apply the same set of techniques to other pretrained models as well. Come check it out here: https://github.com/huggingface/peft ü§ó\n",
        "\n",
        "Don't forget to tweet your results and tag us! [@huggingface](https://twitter.com/huggingface) and [@reach_vb](https://twitter.com/reach_vb) ‚ù§Ô∏è"
      ],
      "metadata": {
        "id": "j3XF0PzsCV0v"
      },
      "id": "j3XF0PzsCV0v"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}